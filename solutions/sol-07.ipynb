{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lo66foonmvxg"
   },
   "source": [
    "# Classification with KNN, Trees and Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "v3kH50wbmjBL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_cnE38km64G"
   },
   "source": [
    "Load and split the data from the Unsupervise Learning Dataset (Lab 5, Dry Bean Dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0eM9w9HnRDf",
    "outputId": "80450ddf-e3bc-41de-8f8e-a0f1ad247c92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n",
      "File is readable\n"
     ]
    }
   ],
   "source": [
    "FFILE = './Dry_Bean_Dataset.xlsx'\n",
    "if os.path.isfile(FFILE):\n",
    "    print(\"File already exists\")\n",
    "    if os.access(FFILE, os.R_OK):\n",
    "        print (\"File is readable\")\n",
    "    else:\n",
    "        print (\"File is not readable, removing it and downloading again\")\n",
    "        !rm FFILE\n",
    "        !wget \"https://raw.github.com/alexdepremia/ML_IADA_UTs/main/Lab5/Dry_Bean_Dataset.xlsx\"\n",
    "else:\n",
    "    print(\"Either the file is missing or not readable, download it\")\n",
    "    !wget \"https://raw.github.com/alexdepremia/ML_IADA_UTs/main/Lab5/Dry_Bean_Dataset.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "CPt8p6GsnTGQ",
    "outputId": "41e2b88b-9ce8-4ecc-ed5c-059cc8a492bf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>MajorAxisLength</th>\n",
       "      <th>MinorAxisLength</th>\n",
       "      <th>AspectRation</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>ConvexArea</th>\n",
       "      <th>EquivDiameter</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Solidity</th>\n",
       "      <th>roundness</th>\n",
       "      <th>Compactness</th>\n",
       "      <th>ShapeFactor1</th>\n",
       "      <th>ShapeFactor2</th>\n",
       "      <th>ShapeFactor3</th>\n",
       "      <th>ShapeFactor4</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28395</td>\n",
       "      <td>610.291</td>\n",
       "      <td>208.178117</td>\n",
       "      <td>173.888747</td>\n",
       "      <td>1.197191</td>\n",
       "      <td>0.549812</td>\n",
       "      <td>28715</td>\n",
       "      <td>190.141097</td>\n",
       "      <td>0.763923</td>\n",
       "      <td>0.988856</td>\n",
       "      <td>0.958027</td>\n",
       "      <td>0.913358</td>\n",
       "      <td>0.007332</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>0.834222</td>\n",
       "      <td>0.998724</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28734</td>\n",
       "      <td>638.018</td>\n",
       "      <td>200.524796</td>\n",
       "      <td>182.734419</td>\n",
       "      <td>1.097356</td>\n",
       "      <td>0.411785</td>\n",
       "      <td>29172</td>\n",
       "      <td>191.272750</td>\n",
       "      <td>0.783968</td>\n",
       "      <td>0.984986</td>\n",
       "      <td>0.887034</td>\n",
       "      <td>0.953861</td>\n",
       "      <td>0.006979</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.909851</td>\n",
       "      <td>0.998430</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29380</td>\n",
       "      <td>624.110</td>\n",
       "      <td>212.826130</td>\n",
       "      <td>175.931143</td>\n",
       "      <td>1.209713</td>\n",
       "      <td>0.562727</td>\n",
       "      <td>29690</td>\n",
       "      <td>193.410904</td>\n",
       "      <td>0.778113</td>\n",
       "      <td>0.989559</td>\n",
       "      <td>0.947849</td>\n",
       "      <td>0.908774</td>\n",
       "      <td>0.007244</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>0.825871</td>\n",
       "      <td>0.999066</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30008</td>\n",
       "      <td>645.884</td>\n",
       "      <td>210.557999</td>\n",
       "      <td>182.516516</td>\n",
       "      <td>1.153638</td>\n",
       "      <td>0.498616</td>\n",
       "      <td>30724</td>\n",
       "      <td>195.467062</td>\n",
       "      <td>0.782681</td>\n",
       "      <td>0.976696</td>\n",
       "      <td>0.903936</td>\n",
       "      <td>0.928329</td>\n",
       "      <td>0.007017</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>0.861794</td>\n",
       "      <td>0.994199</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30140</td>\n",
       "      <td>620.134</td>\n",
       "      <td>201.847882</td>\n",
       "      <td>190.279279</td>\n",
       "      <td>1.060798</td>\n",
       "      <td>0.333680</td>\n",
       "      <td>30417</td>\n",
       "      <td>195.896503</td>\n",
       "      <td>0.773098</td>\n",
       "      <td>0.990893</td>\n",
       "      <td>0.984877</td>\n",
       "      <td>0.970516</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>0.003665</td>\n",
       "      <td>0.941900</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Area  Perimeter  MajorAxisLength  MinorAxisLength  AspectRation  \\\n",
       "0  28395    610.291       208.178117       173.888747      1.197191   \n",
       "1  28734    638.018       200.524796       182.734419      1.097356   \n",
       "2  29380    624.110       212.826130       175.931143      1.209713   \n",
       "3  30008    645.884       210.557999       182.516516      1.153638   \n",
       "4  30140    620.134       201.847882       190.279279      1.060798   \n",
       "\n",
       "   Eccentricity  ConvexArea  EquivDiameter    Extent  Solidity  roundness  \\\n",
       "0      0.549812       28715     190.141097  0.763923  0.988856   0.958027   \n",
       "1      0.411785       29172     191.272750  0.783968  0.984986   0.887034   \n",
       "2      0.562727       29690     193.410904  0.778113  0.989559   0.947849   \n",
       "3      0.498616       30724     195.467062  0.782681  0.976696   0.903936   \n",
       "4      0.333680       30417     195.896503  0.773098  0.990893   0.984877   \n",
       "\n",
       "   Compactness  ShapeFactor1  ShapeFactor2  ShapeFactor3  ShapeFactor4  Class  \n",
       "0     0.913358      0.007332      0.003147      0.834222      0.998724  SEKER  \n",
       "1     0.953861      0.006979      0.003564      0.909851      0.998430  SEKER  \n",
       "2     0.908774      0.007244      0.003048      0.825871      0.999066  SEKER  \n",
       "3     0.928329      0.007017      0.003215      0.861794      0.994199  SEKER  \n",
       "4     0.970516      0.006697      0.003665      0.941900      0.999166  SEKER  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "data = pd.read_excel('./Dry_Bean_Dataset.xlsx', engine='openpyxl')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5m3HOy0nny0"
   },
   "source": [
    "Divide features and label. Split the data in train and test set and **after that** normalize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ZpAplcpvnunq",
    "outputId": "652e5d72-e69d-4349-de38-254c9b06199a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>MajorAxisLength</th>\n",
       "      <th>MinorAxisLength</th>\n",
       "      <th>AspectRation</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>ConvexArea</th>\n",
       "      <th>EquivDiameter</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Solidity</th>\n",
       "      <th>roundness</th>\n",
       "      <th>Compactness</th>\n",
       "      <th>ShapeFactor1</th>\n",
       "      <th>ShapeFactor2</th>\n",
       "      <th>ShapeFactor3</th>\n",
       "      <th>ShapeFactor4</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37277</td>\n",
       "      <td>710.193</td>\n",
       "      <td>264.789840</td>\n",
       "      <td>179.808422</td>\n",
       "      <td>1.472622</td>\n",
       "      <td>0.734082</td>\n",
       "      <td>37684</td>\n",
       "      <td>217.859015</td>\n",
       "      <td>0.802692</td>\n",
       "      <td>0.989200</td>\n",
       "      <td>0.928748</td>\n",
       "      <td>0.822762</td>\n",
       "      <td>0.007103</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>0.676937</td>\n",
       "      <td>0.996873</td>\n",
       "      <td>DERMASON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28942</td>\n",
       "      <td>638.821</td>\n",
       "      <td>239.861192</td>\n",
       "      <td>154.004371</td>\n",
       "      <td>1.557496</td>\n",
       "      <td>0.766658</td>\n",
       "      <td>29368</td>\n",
       "      <td>191.963796</td>\n",
       "      <td>0.786126</td>\n",
       "      <td>0.985494</td>\n",
       "      <td>0.891210</td>\n",
       "      <td>0.800312</td>\n",
       "      <td>0.008288</td>\n",
       "      <td>0.002097</td>\n",
       "      <td>0.640499</td>\n",
       "      <td>0.997575</td>\n",
       "      <td>DERMASON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38290</td>\n",
       "      <td>719.888</td>\n",
       "      <td>270.446510</td>\n",
       "      <td>180.508066</td>\n",
       "      <td>1.498252</td>\n",
       "      <td>0.744659</td>\n",
       "      <td>38605</td>\n",
       "      <td>220.799326</td>\n",
       "      <td>0.759903</td>\n",
       "      <td>0.991840</td>\n",
       "      <td>0.928465</td>\n",
       "      <td>0.816425</td>\n",
       "      <td>0.007063</td>\n",
       "      <td>0.001936</td>\n",
       "      <td>0.666550</td>\n",
       "      <td>0.998660</td>\n",
       "      <td>DERMASON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37641</td>\n",
       "      <td>742.538</td>\n",
       "      <td>284.313737</td>\n",
       "      <td>169.740814</td>\n",
       "      <td>1.674987</td>\n",
       "      <td>0.802227</td>\n",
       "      <td>38112</td>\n",
       "      <td>218.920099</td>\n",
       "      <td>0.744187</td>\n",
       "      <td>0.987642</td>\n",
       "      <td>0.857894</td>\n",
       "      <td>0.769995</td>\n",
       "      <td>0.007553</td>\n",
       "      <td>0.001638</td>\n",
       "      <td>0.592892</td>\n",
       "      <td>0.993087</td>\n",
       "      <td>SIRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50172</td>\n",
       "      <td>828.968</td>\n",
       "      <td>316.453571</td>\n",
       "      <td>202.268818</td>\n",
       "      <td>1.564520</td>\n",
       "      <td>0.769062</td>\n",
       "      <td>50547</td>\n",
       "      <td>252.746858</td>\n",
       "      <td>0.688240</td>\n",
       "      <td>0.992581</td>\n",
       "      <td>0.917478</td>\n",
       "      <td>0.798685</td>\n",
       "      <td>0.006307</td>\n",
       "      <td>0.001583</td>\n",
       "      <td>0.637898</td>\n",
       "      <td>0.998005</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Area  Perimeter  MajorAxisLength  MinorAxisLength  AspectRation  \\\n",
       "0  37277    710.193       264.789840       179.808422      1.472622   \n",
       "1  28942    638.821       239.861192       154.004371      1.557496   \n",
       "2  38290    719.888       270.446510       180.508066      1.498252   \n",
       "3  37641    742.538       284.313737       169.740814      1.674987   \n",
       "4  50172    828.968       316.453571       202.268818      1.564520   \n",
       "\n",
       "   Eccentricity  ConvexArea  EquivDiameter    Extent  Solidity  roundness  \\\n",
       "0      0.734082       37684     217.859015  0.802692  0.989200   0.928748   \n",
       "1      0.766658       29368     191.963796  0.786126  0.985494   0.891210   \n",
       "2      0.744659       38605     220.799326  0.759903  0.991840   0.928465   \n",
       "3      0.802227       38112     218.920099  0.744187  0.987642   0.857894   \n",
       "4      0.769062       50547     252.746858  0.688240  0.992581   0.917478   \n",
       "\n",
       "   Compactness  ShapeFactor1  ShapeFactor2  ShapeFactor3  ShapeFactor4  \\\n",
       "0     0.822762      0.007103      0.002008      0.676937      0.996873   \n",
       "1     0.800312      0.008288      0.002097      0.640499      0.997575   \n",
       "2     0.816425      0.007063      0.001936      0.666550      0.998660   \n",
       "3     0.769995      0.007553      0.001638      0.592892      0.993087   \n",
       "4     0.798685      0.006307      0.001583      0.637898      0.998005   \n",
       "\n",
       "      Class  \n",
       "0  DERMASON  \n",
       "1  DERMASON  \n",
       "2  DERMASON  \n",
       "3      SIRA  \n",
       "4     SEKER  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.sample(frac=1,random_state=0).reset_index(drop=True) # random shuffle\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pLpLk6q7NTNQ",
    "outputId": "19bc6c0d-96b0-4928-c95a-16d83c8d7ecd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13611, 17)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DERMASON', 'SIRA', 'SEKER', 'CALI', 'BOMBAY', 'HOROZ', 'BARBUNYA'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "K9I8tR3gZanQ"
   },
   "outputs": [],
   "source": [
    "train_data = data.iloc[:10000,:]\n",
    "test_data = data.iloc[10000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w4Sai6BSZkWf",
    "outputId": "ab8ce9d4-fccd-4e0a-8c20-4620f4d14a1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 17)\n",
      "(3611, 17)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tIl6l3KXZsO3"
   },
   "outputs": [],
   "source": [
    "# normalize train and test dataset\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Extract the class labels from the training dataset\n",
    "label_train = train_data['Class']\n",
    "# Remove the class labels from the training dataset\n",
    "train_data = train_data.drop('Class', axis=1)\n",
    "# Save the column names for later use\n",
    "columns_name = train_data.columns\n",
    "\n",
    "# Initialize a StandardScaler and fit it to the training data\n",
    "train_scaler = preprocessing.StandardScaler().fit(train_data)\n",
    "# Transform the training data using the scaler\n",
    "train_data = train_scaler.transform(train_data)\n",
    "# Create a DataFrame with the scaled training data and restore column names\n",
    "train_data = pd.DataFrame(train_data, columns=columns_name)\n",
    "# Add the class labels back to the scaled training dataset\n",
    "train_data['Class'] = label_train\n",
    "\n",
    "# Extract the class labels from the test dataset\n",
    "label_test = test_data['Class']\n",
    "# Remove the class labels from the test dataset\n",
    "test_data = test_data.drop('Class', axis=1)\n",
    "# Initialize a StandardScaler and fit it to the test data\n",
    "test_scaler = preprocessing.StandardScaler().fit(test_data)\n",
    "# Transform the test data using the scaler\n",
    "test_data = test_scaler.transform(test_data)\n",
    "# Create a DataFrame with the scaled test data and restore column names\n",
    "test_data = pd.DataFrame(test_data, columns=columns_name)\n",
    "# Add the class labels back to the scaled test dataset\n",
    "test_data['Class'] = label_test.values # .values added to prevent nans to appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "WdaI84S-eDNZ",
    "outputId": "0f0749f3-f5f0-4c55-c400-5aa7becfeeda"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>MajorAxisLength</th>\n",
       "      <th>MinorAxisLength</th>\n",
       "      <th>AspectRation</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>ConvexArea</th>\n",
       "      <th>EquivDiameter</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Solidity</th>\n",
       "      <th>roundness</th>\n",
       "      <th>Compactness</th>\n",
       "      <th>ShapeFactor1</th>\n",
       "      <th>ShapeFactor2</th>\n",
       "      <th>ShapeFactor3</th>\n",
       "      <th>ShapeFactor4</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.534137</td>\n",
       "      <td>-0.673988</td>\n",
       "      <td>-0.642784</td>\n",
       "      <td>-0.494897</td>\n",
       "      <td>-0.448854</td>\n",
       "      <td>-0.185185</td>\n",
       "      <td>-0.536571</td>\n",
       "      <td>-0.590932</td>\n",
       "      <td>1.083156</td>\n",
       "      <td>0.441530</td>\n",
       "      <td>0.932328</td>\n",
       "      <td>0.372606</td>\n",
       "      <td>0.473404</td>\n",
       "      <td>0.490160</td>\n",
       "      <td>0.338721</td>\n",
       "      <td>0.416048</td>\n",
       "      <td>DERMASON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.819126</td>\n",
       "      <td>-1.008115</td>\n",
       "      <td>-0.934618</td>\n",
       "      <td>-1.070089</td>\n",
       "      <td>-0.104138</td>\n",
       "      <td>0.170622</td>\n",
       "      <td>-0.816632</td>\n",
       "      <td>-1.029793</td>\n",
       "      <td>0.746572</td>\n",
       "      <td>-0.348554</td>\n",
       "      <td>0.302277</td>\n",
       "      <td>0.007584</td>\n",
       "      <td>1.526541</td>\n",
       "      <td>0.640814</td>\n",
       "      <td>-0.030790</td>\n",
       "      <td>0.576165</td>\n",
       "      <td>DERMASON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.499500</td>\n",
       "      <td>-0.628601</td>\n",
       "      <td>-0.576563</td>\n",
       "      <td>-0.479301</td>\n",
       "      <td>-0.344759</td>\n",
       "      <td>-0.069652</td>\n",
       "      <td>-0.505554</td>\n",
       "      <td>-0.541101</td>\n",
       "      <td>0.213791</td>\n",
       "      <td>1.004632</td>\n",
       "      <td>0.927566</td>\n",
       "      <td>0.269572</td>\n",
       "      <td>0.437664</td>\n",
       "      <td>0.368508</td>\n",
       "      <td>0.233385</td>\n",
       "      <td>0.823378</td>\n",
       "      <td>DERMASON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.521691</td>\n",
       "      <td>-0.522565</td>\n",
       "      <td>-0.414222</td>\n",
       "      <td>-0.719312</td>\n",
       "      <td>0.373055</td>\n",
       "      <td>0.559127</td>\n",
       "      <td>-0.522157</td>\n",
       "      <td>-0.572949</td>\n",
       "      <td>-0.105517</td>\n",
       "      <td>0.109317</td>\n",
       "      <td>-0.256904</td>\n",
       "      <td>-0.485355</td>\n",
       "      <td>0.873546</td>\n",
       "      <td>-0.133675</td>\n",
       "      <td>-0.513569</td>\n",
       "      <td>-0.447017</td>\n",
       "      <td>SIRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.093232</td>\n",
       "      <td>-0.117944</td>\n",
       "      <td>-0.037969</td>\n",
       "      <td>0.005762</td>\n",
       "      <td>-0.075610</td>\n",
       "      <td>0.196889</td>\n",
       "      <td>-0.103380</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>-1.242245</td>\n",
       "      <td>1.162580</td>\n",
       "      <td>0.743167</td>\n",
       "      <td>-0.018863</td>\n",
       "      <td>-0.234347</td>\n",
       "      <td>-0.225790</td>\n",
       "      <td>-0.057166</td>\n",
       "      <td>0.674089</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Area  Perimeter  MajorAxisLength  MinorAxisLength  AspectRation  \\\n",
       "0 -0.534137  -0.673988        -0.642784        -0.494897     -0.448854   \n",
       "1 -0.819126  -1.008115        -0.934618        -1.070089     -0.104138   \n",
       "2 -0.499500  -0.628601        -0.576563        -0.479301     -0.344759   \n",
       "3 -0.521691  -0.522565        -0.414222        -0.719312      0.373055   \n",
       "4 -0.093232  -0.117944        -0.037969         0.005762     -0.075610   \n",
       "\n",
       "   Eccentricity  ConvexArea  EquivDiameter    Extent  Solidity  roundness  \\\n",
       "0     -0.185185   -0.536571      -0.590932  1.083156  0.441530   0.932328   \n",
       "1      0.170622   -0.816632      -1.029793  0.746572 -0.348554   0.302277   \n",
       "2     -0.069652   -0.505554      -0.541101  0.213791  1.004632   0.927566   \n",
       "3      0.559127   -0.522157      -0.572949 -0.105517  0.109317  -0.256904   \n",
       "4      0.196889   -0.103380       0.000331 -1.242245  1.162580   0.743167   \n",
       "\n",
       "   Compactness  ShapeFactor1  ShapeFactor2  ShapeFactor3  ShapeFactor4  \\\n",
       "0     0.372606      0.473404      0.490160      0.338721      0.416048   \n",
       "1     0.007584      1.526541      0.640814     -0.030790      0.576165   \n",
       "2     0.269572      0.437664      0.368508      0.233385      0.823378   \n",
       "3    -0.485355      0.873546     -0.133675     -0.513569     -0.447017   \n",
       "4    -0.018863     -0.234347     -0.225790     -0.057166      0.674089   \n",
       "\n",
       "      Class  \n",
       "0  DERMASON  \n",
       "1  DERMASON  \n",
       "2  DERMASON  \n",
       "3      SIRA  \n",
       "4     SEKER  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DERMASON', 'SEKER', 'SIRA', 'BARBUNYA', 'HOROZ', 'BOMBAY', 'CALI'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['Class'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxDH5MYE21sj"
   },
   "source": [
    "**Before feeding the data into the following algorithms, try to perform PCA, varying the number of PCs, and check what changes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FptPlXMAnuym"
   },
   "source": [
    "## K-Nearest Neighbors Classification\n",
    "\n",
    "Implement the KNN algorithm for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "urjD3tGR3DV7"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean, minkowski\n",
    "from tqdm import tqdm\n",
    "\n",
    "def distance(point_one, point_two, dist, p):\n",
    "    \"\"\"\n",
    "    Calculate the Euclidean distance between two points.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    point_one : array-like\n",
    "        Coordinates of the first point.\n",
    "    point_two : array-like\n",
    "        Coordinates of the second point.\n",
    "    dist: str\n",
    "        Allow to choose between Euclidean or Minkowski distance.\n",
    "    p: int\n",
    "        Order of the norm, only used with Minkowski distance.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Euclidean or Minkowski distance between the two points.\n",
    "    \"\"\"\n",
    "    if dist == 'euclidean':\n",
    "        return euclidean(point_one, point_two)\n",
    "    else:\n",
    "        return minkowski(point_one, point_two, p=p)\n",
    "\n",
    "\n",
    "def get_neighbors(train_set, test_point, label_col, n_neighbors, dist, p):\n",
    "    \"\"\"\n",
    "    Get the nearest neighbors of a test point in the training set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_set : array-like\n",
    "        The training set containing data points.\n",
    "    test_point : array-like\n",
    "        The test point for which neighbors are to be found.\n",
    "    label_col : array-like\n",
    "        The labels corresponding to the training set.\n",
    "    n_neighbors : int\n",
    "        The number of neighbors to retrieve.\n",
    "    dist: str\n",
    "        Allow to choose between Euclidean or Minkowski distance.\n",
    "    p: int\n",
    "        Order of the norm, only used with Minkowski distance.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ordered_train : array-like\n",
    "        The nearest neighbors in the training set.\n",
    "    ordered_label : array-like\n",
    "        The corresponding labels of the nearest neighbors.\n",
    "    \"\"\"\n",
    "    # Calculate distances between the test point and all points in the training set\n",
    "    dist = np.array([distance(train_point, test_point, dist, p) for train_point in train_set])\n",
    "    # Get indices that would sort the distances in ascending order\n",
    "    idx_dist = dist.argsort()\n",
    "    # Order the training set and labels based on the sorted distances\n",
    "    ordered_train = train_set[idx_dist, :]\n",
    "    ordered_label = label_col[idx_dist]\n",
    "    # Return the top n_neighbors neighbors and their labels\n",
    "    return ordered_train[:n_neighbors], ordered_label[:n_neighbors]\n",
    "\n",
    "def predict(train_set, test_point, labels, n_neighbors, dist, p):\n",
    "    \"\"\"\n",
    "    Predict the label of a test point using k-nearest neighbors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_set : array-like\n",
    "        The training set containing data points.\n",
    "    test_point : array-like\n",
    "        The test point for which the label is to be predicted.\n",
    "    labels : array-like\n",
    "        The labels corresponding to the training set.\n",
    "    n_neighbors : int\n",
    "        The number of neighbors to consider for the prediction.\n",
    "    dist: str\n",
    "        Allow to choose between Euclidean or Minkowski distance.\n",
    "    p: int\n",
    "        Order of the norm, only used with Minkowski distance.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predicted_label : array-like\n",
    "        The predicted label for the test point.\n",
    "    \"\"\"\n",
    "    # Get the nearest neighbors and their labels\n",
    "    neigh, neigh_label = get_neighbors(train_set, test_point, labels, n_neighbors, dist, p)\n",
    "    # Count occurrences of each label among the neighbors\n",
    "    values, counts = np.unique(neigh_label, return_counts=True)\n",
    "    # Find the label with the highest count (majority class)\n",
    "    idx = np.argmax(counts)\n",
    "    # Return the predicted label\n",
    "    return values[idx]\n",
    "\n",
    "def evaluate(train_set, test_set, label, n_neighbors=2, dist='Euclidean', p=2):\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of k-nearest neighbors algorithm on a test set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_set : DataFrame\n",
    "        The training dataset.\n",
    "    test_set : DataFrame\n",
    "        The test dataset.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "    n_neighbors : int, optional\n",
    "        The number of neighbors to consider for the prediction. Default is 2.\n",
    "    dist: str\n",
    "        Allow to choose between Euclidean or Minkowski distance.\n",
    "    p: int\n",
    "        Order of the norm, only used with Minkowski distance.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    accuracy : float\n",
    "        The accuracy of the k-nearest neighbors algorithm on the test set.\n",
    "    \"\"\"\n",
    "    # Initialize counters for correct and incorrect predictions\n",
    "    correct_predict = 0\n",
    "    wrong_predict = 0\n",
    "    # Extract labels and features from the training and test sets\n",
    "    \n",
    "    train_labels = train_set[label].values\n",
    "    train_set = train_set.drop(label, axis=1)\n",
    "    \n",
    "    test_labels = test_set[label].values\n",
    "    test_set = test_set.drop(label, axis=1)\n",
    "    # Iterate through each row in the test dataset\n",
    "    for index in tqdm(range(len(test_set.index)), desc=\"Evaluating KNN\"):\n",
    "        # Predict the class label for the current test row\n",
    "        result = predict(train_set.values, test_set.iloc[index].values, train_labels, n_neighbors, dist, p)\n",
    "        # Check if the predicted value matches the actual value\n",
    "        if result == test_labels[index]:\n",
    "            # Increase the correct prediction count\n",
    "            correct_predict += 1\n",
    "        else:\n",
    "            # Increase the incorrect prediction count\n",
    "            wrong_predict += 1\n",
    "\n",
    "    # Calculate and return the accuracy\n",
    "    accuracy = correct_predict / (correct_predict + wrong_predict)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cOUsnIsElYNA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating KNN: 100%|██████████| 3611/3611 [04:34<00:00, 13.15it/s]\n"
     ]
    }
   ],
   "source": [
    "knn_accuracy = evaluate(train_data, test_data, 'Class', n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DdRfqW8mT25Z",
    "outputId": "2ce2c62a-1824-4205-f55f-fb088101eaae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9243976737745777"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZd_CqNr3DnI"
   },
   "source": [
    "## Decision Trees with Numerical Features\n",
    "\n",
    "Modify the implementation of decision trees to account for numerical input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2yVWtuSI3Q07"
   },
   "outputs": [],
   "source": [
    "# compute H(S)\n",
    "def entropy(train_data, label, class_list):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : DataFrame\n",
    "        The training dataset.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "    class_list : list of str\n",
    "        List of possible values of the class labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    total_entr : float\n",
    "        The entropy of the dataset.\n",
    "    \"\"\"\n",
    "    # Get the total number of instances in the dataset\n",
    "    total_row = train_data.shape[0]\n",
    "    # Initialize the total entropy variable\n",
    "    total_entr = 0\n",
    "\n",
    "    # Iterate through each possible class in the label\n",
    "    for c in class_list:\n",
    "        # Count the number of points belonging to the current class\n",
    "        total_class_count = train_data[train_data[label] == c].shape[0]\n",
    "\n",
    "        # Check if there are instances of the class to avoid numerical errors\n",
    "        if total_class_count > 0:\n",
    "            # Calculate the entropy of the current class\n",
    "            total_class_entr = - (total_class_count / total_row) * np.log2(total_class_count / total_row)\n",
    "            # Add the entropy of the current class to the total entropy of the dataset\n",
    "            total_entr += total_class_entr\n",
    "\n",
    "    # Return the calculated total entropy of the dataset\n",
    "    return total_entr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "bAF8R0lT03Um"
   },
   "outputs": [],
   "source": [
    "# compute H(S_j)\n",
    "def feature_entropy(left_data, right_data, label, class_list):\n",
    "    \"\"\"\n",
    "    Calculate the conditional entropy of a dataset split by a specific feature.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    left_data : DataFrame\n",
    "        Subset of the dataset where the feature has a specific value.\n",
    "    right_data : DataFrame\n",
    "        Subset of the dataset where the feature has another value.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "    class_list : list of str\n",
    "        List of possible values of the class labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ent : float\n",
    "        The conditional entropy of the dataset split by the feature.\n",
    "    \"\"\"\n",
    "    # Get the total number of points considered after the split\n",
    "    row_count = left_data.shape[0] + right_data.shape[0]\n",
    "\n",
    "    # Calculate the probabilities of the left and right subsets\n",
    "    p_left = left_data.shape[0] / row_count\n",
    "    p_right = right_data.shape[0] / row_count\n",
    "\n",
    "    # Calculate the conditional entropy using the weighted average of entropies for left and right subsets\n",
    "    ent = p_left * entropy(left_data, label, class_list) + p_right * entropy(right_data, label, class_list)\n",
    "\n",
    "    # Return the calculated conditional entropy\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "caDIpINC86M3"
   },
   "outputs": [],
   "source": [
    "def split(feature_column, threshold):\n",
    "    \"\"\"\n",
    "    Split the indices of data points based on a feature and a threshold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_column : array-like\n",
    "        The values of the feature for each data point.\n",
    "    threshold : float\n",
    "        The threshold value for splitting the data points.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    left_rows : array-like\n",
    "        Indices of data points where the feature value is less than or equal to the threshold.\n",
    "    right_rows : array-like\n",
    "        Indices of data points where the feature value is greater than the threshold.\n",
    "    \"\"\"\n",
    "    # Find the indices of data points where the feature value is less than or equal to the threshold\n",
    "    left_rows = np.argwhere(feature_column <= threshold).flatten()\n",
    "    # Find the indices of data points where the feature value is greater than the threshold\n",
    "    right_rows = np.argwhere(feature_column > threshold).flatten()\n",
    "\n",
    "    # Return the indices for left and right subsets\n",
    "    return left_rows, right_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NLv85AnDUO3X"
   },
   "outputs": [],
   "source": [
    "def information_gain(data, feature_name, label, class_list, threshold):\n",
    "    \"\"\"\n",
    "    Calculate the information gain after splitting the dataset based on a feature and a threshold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : DataFrame\n",
    "        The dataset.\n",
    "    feature_name : str\n",
    "        The name of the feature for which information gain is calculated.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "    class_list : list of str\n",
    "        List of possible values of the class labels.\n",
    "    threshold : float\n",
    "        The threshold value for splitting the dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    feat_information_gain : float\n",
    "        The information gain achieved by splitting the dataset based on the specified feature and threshold.\n",
    "    \"\"\"\n",
    "    # Split the dataset into left and right subsets based on the feature and threshold\n",
    "    left_rows, right_rows = split(data[feature_name].values, threshold)\n",
    "\n",
    "    # Check if either subset is empty; if so, information gain is zero\n",
    "    if len(left_rows) == 0 or len(right_rows) == 0:\n",
    "        return 0\n",
    "\n",
    "    # Calculate the entropy of the split dataset\n",
    "    feat_entropy = feature_entropy(data.iloc[left_rows], data.iloc[right_rows], label, class_list)\n",
    "\n",
    "    return feat_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Xzd4n-amVKzR"
   },
   "outputs": [],
   "source": [
    "def get_split_thresholds(feature_column, n_thresholds):\n",
    "    \"\"\"\n",
    "    Generate candidate split thresholds for a given feature column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_column : array-like\n",
    "        The values of the feature for each data point.\n",
    "    n_thresholds : int\n",
    "        The number of thresholds to generate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    thresholds : list of float\n",
    "        List of candidate split thresholds for the feature column.\n",
    "    \"\"\"\n",
    "    # Extract the values of the feature column\n",
    "    feature_column = feature_column.values\n",
    "    # Get the total number of data points\n",
    "    n_data = len(feature_column)\n",
    "\n",
    "    # Sort the feature column in ascending order\n",
    "    sorted_column = np.sort(feature_column)\n",
    "\n",
    "    # Check if there is more than one data point\n",
    "    if len(feature_column) > 1:\n",
    "        # Split the sorted feature column into n_thresholds + 1 partitions\n",
    "        partitioned_array = np.array_split(sorted_column, n_thresholds + 1)\n",
    "\n",
    "        # Calculate the midpoint between consecutive partitions as candidate thresholds\n",
    "        thresholds = [(partitioned_array[i][-1] + partitioned_array[i + 1][0]) / 2 for i in range(len(partitioned_array) - 1)]\n",
    "    else:\n",
    "        # If there is only one data point, use it as the threshold\n",
    "        thresholds = [feature_column[0]]\n",
    "\n",
    "    # Return the list of candidate split thresholds\n",
    "    return thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "TS0nppcLVex3"
   },
   "outputs": [],
   "source": [
    "def most_informative_feature(train_data, label, class_list, n_thresholds):\n",
    "    \"\"\"\n",
    "    Find the most informative feature and its corresponding threshold for splitting the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : DataFrame\n",
    "        The training dataset.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "    class_list : list of str\n",
    "        List of possible values of the class labels.\n",
    "    n_thresholds : int\n",
    "        The number of thresholds to generate for each feature.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    min_entropy_feature : str\n",
    "        The name of the most informative feature.\n",
    "    min_entropy_threshold : float\n",
    "        The corresponding threshold for splitting the dataset based on the most informative feature.\n",
    "    \"\"\"\n",
    "    # Get the list of features excluding the label\n",
    "    feature_list = train_data.columns.drop(label)\n",
    "\n",
    "    # Initialize variables to store the minimum entropy and corresponding feature and threshold\n",
    "    min_entropy = float('inf')\n",
    "    min_entropy_feature = None\n",
    "    min_entropy_threshold = None\n",
    "\n",
    "    # Iterate over each feature in the feature list\n",
    "    for feature in feature_list:\n",
    "        # Generate candidate split thresholds for the current feature\n",
    "        thresholds = get_split_thresholds(train_data[feature], n_thresholds)\n",
    "\n",
    "        # Iterate over each threshold\n",
    "        for t in thresholds:\n",
    "            # Calculate information gain for the current feature and threshold\n",
    "            info_gain = information_gain(train_data, feature, label, class_list, t)\n",
    "\n",
    "            # Check if the calculated information gain is less than the current minimum entropy\n",
    "            if info_gain < min_entropy:\n",
    "                # Update the minimum entropy and corresponding feature and threshold\n",
    "                min_entropy = info_gain\n",
    "                min_entropy_feature = feature\n",
    "                min_entropy_threshold = t\n",
    "\n",
    "    # Return the most informative feature and its corresponding threshold\n",
    "    return min_entropy_feature, min_entropy_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "SFkkyLqj67Ce"
   },
   "outputs": [],
   "source": [
    "def is_leaf(train_data, label):\n",
    "    \"\"\"\n",
    "    Check if a node in a decision tree is a leaf node.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : DataFrame\n",
    "        The dataset associated with the current node.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the node is a leaf node (contains only one class), False otherwise.\n",
    "    \"\"\"\n",
    "    # Get the unique classes in the current node\n",
    "    classes_in_node = np.unique(train_data[label])\n",
    "\n",
    "    # Check if there is only one class in the node\n",
    "    if len(classes_in_node) == 1:\n",
    "        # If there is only one class, the node is a leaf node\n",
    "        return True\n",
    "    else:\n",
    "        # If there is more than one class, the node is not a leaf node\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "nmSk5xUd8SgM"
   },
   "outputs": [],
   "source": [
    "def leaf_class(train_data, label):\n",
    "    \"\"\"\n",
    "    Determine the class of a leaf node in a decision tree.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : DataFrame\n",
    "        The dataset associated with the leaf node.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    leaf_class : str\n",
    "        The class label assigned to the leaf node.\n",
    "    \"\"\"\n",
    "    # Get the unique classes and their counts in the current leaf node\n",
    "    class_list, count_class = np.unique(train_data[label], return_counts=True)\n",
    "\n",
    "    # Find the index of the class with the highest count (most frequent class)\n",
    "    idx = count_class.argmax()\n",
    "\n",
    "    # Return the class label associated with the most frequent class in the leaf node\n",
    "    return class_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "TbRY5klnWCMP"
   },
   "outputs": [],
   "source": [
    "def make_tree(train_data, label, class_list, n_thresholds, cur_depth, min_samples, max_depth):\n",
    "    \"\"\"\n",
    "    Recursively build a decision tree.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : DataFrame\n",
    "        The training dataset associated with the current node.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "    class_list : list of str\n",
    "        List of possible values of the class labels.\n",
    "    n_thresholds : int\n",
    "        The number of thresholds to generate for each feature.\n",
    "    cur_depth : int\n",
    "        The current depth of the decision tree.\n",
    "    min_samples : int\n",
    "        The minimum number of samples required to split a node.\n",
    "    max_depth : int\n",
    "        The maximum depth of the decision tree.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tree : dict or str\n",
    "        The constructed decision tree represented as a nested dictionary. If a leaf node, returns the class label.\n",
    "    \"\"\"\n",
    "    # Check stopping conditions for creating a leaf node\n",
    "    if is_leaf(train_data, label) or cur_depth >= max_depth or len(train_data) <= min_samples:\n",
    "        return leaf_class(train_data, label)\n",
    "    else:\n",
    "        # Increment the current depth for the next level of recursion\n",
    "        cur_depth += 1\n",
    "\n",
    "        # Find the most informative feature and its corresponding threshold for splitting\n",
    "        split_feature, split_threshold = most_informative_feature(train_data, label, class_list, n_thresholds)\n",
    "\n",
    "        # Split the dataset into left and right subsets based on the feature and threshold\n",
    "        left_rows, right_rows = split(train_data[split_feature].values, split_threshold)\n",
    "\n",
    "        # Check if either subset is empty; if so, create a leaf node\n",
    "        if len(left_rows) == 0 or len(right_rows) == 0:\n",
    "            return leaf_class(train_data, label)\n",
    "        else:\n",
    "            # Build the subtree\n",
    "            split_condition = \"{} <= {}\".format(split_feature, split_threshold)\n",
    "            sub_tree = {split_condition: []}\n",
    "\n",
    "            # Recursive calls for the left and right branches\n",
    "            left_branch = make_tree(train_data.iloc[left_rows], label, class_list, n_thresholds, cur_depth, min_samples, max_depth)\n",
    "            right_branch = make_tree(train_data.iloc[right_rows], label, class_list, n_thresholds, cur_depth, min_samples, max_depth)\n",
    "\n",
    "            # Check if both branches result in the same leaf class; if so, make the subtree a leaf\n",
    "            if left_branch == right_branch:\n",
    "                sub_tree = left_branch\n",
    "            else:\n",
    "                # Grow the tree by adding left and right branches to the split condition\n",
    "                sub_tree[split_condition].append(left_branch)\n",
    "                sub_tree[split_condition].append(right_branch)\n",
    "\n",
    "            return sub_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "DrEcz2r31Mek"
   },
   "outputs": [],
   "source": [
    "def id3(train_data_m, label, n_thresholds=1, min_samples=4, max_depth=6):\n",
    "    \"\"\"\n",
    "    Build a decision tree using the ID3 algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data_m : DataFrame\n",
    "        The training dataset.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "    n_thresholds : int, optional\n",
    "        The number of thresholds to generate for each feature.\n",
    "    min_samples : int, optional\n",
    "        The minimum number of samples required to split a node.\n",
    "    max_depth : int, optional\n",
    "        The maximum depth of the decision tree.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tree : dict or str\n",
    "        The constructed decision tree represented as a nested dictionary. If a leaf node, returns the class label.\n",
    "    \"\"\"\n",
    "    # Create a copy of the training dataset\n",
    "    train_data = train_data_m.copy()\n",
    "\n",
    "    # Get the unique classes of the label\n",
    "    class_list = train_data[label].unique()\n",
    "\n",
    "    # Start the recursion by calling the make_tree function\n",
    "    tree = make_tree(train_data, label, class_list, n_thresholds, 0, min_samples, max_depth)\n",
    "\n",
    "    # Return the constructed decision tree\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9qYx-Fk6CAMF",
    "outputId": "2ea66f87-f0b4-402d-d1ea-c798041fb372"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MajorAxisLength <= -0.26850534253244707': [{'MinorAxisLength <= -0.48101943425512017': [{'Area <= -0.7284149650882628': ['DERMASON', {'Perimeter <= -0.7436366901340172': ['DERMASON', {'roundness <= 0.34196234239628087': [{'ShapeFactor1 <= 0.742560155983303': ['SIRA', 'DERMASON']}, 'DERMASON']}]}]}, {'AspectRation <= -1.0816890411884459': ['SEKER', {'ShapeFactor2 <= 0.30636007444314095': ['SIRA', {'ShapeFactor1 <= 0.14890193826388914': [{'Compactness <= 0.8530383347908611': ['SIRA', 'SEKER']}, 'DERMASON']}]}]}]}, {'ShapeFactor1 <= -0.4188395892843162': [{'ShapeFactor2 <= -0.970259723740702': [{'Perimeter <= 1.2423181848501317': ['CALI', {'ShapeFactor1 <= -2.601011987728354': ['BOMBAY', 'CALI']}]}, {'roundness <= -0.655896778746935': ['BARBUNYA', {'Compactness <= -0.19114716081240862': ['CALI', {'roundness <= -0.07128514945601658': ['BARBUNYA', 'CALI']}]}]}]}, {'Compactness <= -1.1840258480155355': ['HOROZ', {'roundness <= 0.019868264666345184': [{'roundness <= -0.33450228559615847': [{'Compactness <= -0.8153488174886201': ['HOROZ', 'SIRA']}, 'SIRA']}, 'SIRA']}]}]}]}\n"
     ]
    }
   ],
   "source": [
    "t = id3(train_data, 'Class')\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "RvbhleJ0We6n"
   },
   "outputs": [],
   "source": [
    "def predict(test_point, tree):\n",
    "    \"\"\"\n",
    "    Predict the class label for a given test point using a decision tree.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_point : Series\n",
    "        The test point for which the class label is predicted.\n",
    "    tree : dict or str\n",
    "        The decision tree used for prediction.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    prediction : str\n",
    "        The predicted class label for the test point.\n",
    "    \"\"\"\n",
    "    # Base case: if the tree is a leaf node (a class label)\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "\n",
    "    # Recursive case: traverse the tree based on feature values\n",
    "    question = list(tree.keys())[0]\n",
    "    attribute, value = question.split(\" <= \")\n",
    "\n",
    "    # Check the condition and follow the appropriate branch\n",
    "    if test_point[attribute] <= float(value):\n",
    "        answer = tree[question][0]\n",
    "    else:\n",
    "        answer = tree[question][1]\n",
    "\n",
    "    # Recursive call on the selected branch\n",
    "    return predict(test_point, answer)\n",
    "\n",
    "\n",
    "def evaluate(tree, test_data, label):\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of a decision tree on a test dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree : dict or str\n",
    "        The decision tree to be evaluated.\n",
    "    test_data : DataFrame\n",
    "        The test dataset.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    accuracy : float\n",
    "        The accuracy of the decision tree on the test dataset.\n",
    "    \"\"\"\n",
    "    correct_predict = 0\n",
    "    wrong_predict = 0\n",
    "\n",
    "    # Iterate over each row in the test dataset\n",
    "    for index in tqdm(range(len(test_data.index))):\n",
    "        # Predict the class label for the current test point\n",
    "        result = predict(test_data.iloc[index], tree)\n",
    "\n",
    "        # Check if the predicted value matches the expected value\n",
    "        if result == test_data[label].iloc[index]:\n",
    "            correct_predict += 1  # Increase correct count\n",
    "        else:\n",
    "            wrong_predict += 1  # Increase incorrect count\n",
    "\n",
    "    # Calculate and return the accuracy\n",
    "    accuracy = correct_predict / (correct_predict + wrong_predict)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YVHYP7puWnDg",
    "outputId": "e42f2ed6-1995-4a29-8d28-e8bc7be278ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3611/3611 [00:00<00:00, 11327.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8773193021323733"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(t, test_data, 'Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eL_3XYt33RH8"
   },
   "source": [
    "## Gaussian Naive Bayes\n",
    "Modify the implementation of naive Bayes to accout for numerical input features. The likelihood of each class ($p(data|class)$) is assumed to be a Gaussian $\\frac{1}{\\sqrt(\\sigma^2 2 \\pi)} \\exp (-\\frac{1}{2} \\frac{(x-\\mu)^2}{\\sigma^2})$, where $\\mu, \\sigma^2$ are the mean and the variance for each class;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "bri49QEpXq1t"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "def prior(train_data: pd.DataFrame, label: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the log prior probabilities for each class in the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : DataFrame\n",
    "        The training dataset.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    priors : array-like\n",
    "        The log prior probabilities for each class.\n",
    "    \"\"\"\n",
    "    # Calculate the prior probabilities for each class\n",
    "    priors = train_data.groupby(by=label).apply(lambda x: len(x) / len(train_data))\n",
    "\n",
    "    # Return the log of the prior probabilities as an array\n",
    "    return np.log(priors).values\n",
    "\n",
    "\n",
    "def mean_variance(train_data: pd.DataFrame, label:str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculate the mean and variance for each feature in the dataset, grouped by class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : DataFrame\n",
    "        The training dataset.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mean : array-like\n",
    "        The mean values for each feature and class.\n",
    "    variance : array-like\n",
    "        The variance values for each feature and class.\n",
    "    \"\"\"\n",
    "    # Calculate the mean values for each feature and class\n",
    "    mean = train_data.groupby(by=label).apply(lambda x: x.mean(axis=0))\n",
    "\n",
    "    # Calculate the variance values for each feature and class\n",
    "    variance = train_data.groupby(by=label).apply(lambda x: x.var(axis=0))\n",
    "\n",
    "    # Return the mean and variance as arrays\n",
    "    return (mean.values, variance.values + 1e-9)\n",
    "\n",
    "\n",
    "def gaussian_density(mean: np.ndarray, variance: np.ndarray, point: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the Gaussian probability density for a given point.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : array-like\n",
    "        The mean values for each feature and class.\n",
    "    variance : array-like\n",
    "        The variance values for each feature and class.\n",
    "    point : array-like\n",
    "        The values of the features for a given point.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    density : array-like\n",
    "        The Gaussian probability density for the given point.\n",
    "    \"\"\"\n",
    "    # Calculate the Gaussian probability density for each feature\n",
    "    d = (1 / np.sqrt(2*np.pi*variance)) * np.exp((-(point - mean)**2) / (2*variance))\n",
    "\n",
    "    # Return the density as an array\n",
    "    return d\n",
    "\n",
    "\n",
    "def train_gaussian_naive_bayes(train_data: pd.DataFrame, label: str) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Train a Gaussian Naive Bayes classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : DataFrame\n",
    "        The training dataset.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : dict\n",
    "        A dictionary containing the parameters of the trained Gaussian Naive Bayes model.\n",
    "    \"\"\"\n",
    "    # Calculate the mean and variance for each feature and class\n",
    "    mean, variance = mean_variance(train_data, label)\n",
    "\n",
    "    # Calculate the log prior probabilities for each class\n",
    "    priors = prior(train_data, label)\n",
    "\n",
    "    # Get unique class labels and their count\n",
    "    unique_labels = sorted(train_data[label].unique())\n",
    "    n_labels = len(unique_labels)\n",
    "\n",
    "    # Construct and return the Gaussian Naive Bayes model\n",
    "    return {'n_labels': n_labels, 'unique_labels': unique_labels, 'n_classes': n_labels, 'mean': mean,\n",
    "            'variance': variance, 'prior': priors}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m3sq6De4qIhM",
    "outputId": "c7ea73d5-4544-4d25-e7b2-856094b31ad8"
   },
   "outputs": [],
   "source": [
    "gaus_bayes = train_gaussian_naive_bayes(train_data, 'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BARBUNYA', 'BOMBAY', 'CALI', 'DERMASON', 'HOROZ', 'SEKER', 'SIRA']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaus_bayes['unique_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0967, 0.0381, 0.1176, 0.2638, 0.1421, 0.147 , 0.1947])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(gaus_bayes['prior'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 16)\n"
     ]
    }
   ],
   "source": [
    "print(gaus_bayes['mean'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "sK09w28tqO_l"
   },
   "outputs": [],
   "source": [
    "def likelihood(point: np.ndarray, mean: np.ndarray, variance: np.ndarray, n_classes: int, n_feat: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the log posterior probabilities for each class given a data point.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    point : array-like\n",
    "        The values of the features for a given data point.\n",
    "    mean : array-like\n",
    "        The mean values for each feature and class.\n",
    "    variance : array-like\n",
    "        The variance values for each feature and class.\n",
    "    class_list : array-like\n",
    "        The unique class labels.\n",
    "    n_classes : int\n",
    "        The number of classes.\n",
    "    n_feat : int\n",
    "        The number of features.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    posteriors : array-like\n",
    "        The log posterior probabilities for each class.\n",
    "    \"\"\"\n",
    "    likelihoods = []\n",
    "    for i in range(n_classes):\n",
    "        likelihood = 0\n",
    "        for j in range(n_feat):\n",
    "            likelihood += np.log(gaussian_density(mean[i][j], variance[i][j], point[j]))\n",
    "        likelihoods.append(likelihood)\n",
    "    return np.array(likelihoods)\n",
    "\n",
    "\n",
    "def predict(test_data: pd.DataFrame, gaus_bayes:Dict[str, np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predict the class labels for a given test dataset using a trained Gaussian Naive Bayes model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_data : DataFrame\n",
    "        The test dataset.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "    gaus_bayes : dict\n",
    "        A dictionary containing the parameters of the trained Gaussian Naive Bayes model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions : array-like\n",
    "        The predicted class labels for the test dataset.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    n_feat = len(test_data.columns) - 1\n",
    "    for i in range(len(test_data)):\n",
    "        pr = gaus_bayes['prior']\n",
    "        like = likelihood(test_data.iloc[i, :-1], gaus_bayes['mean'], gaus_bayes['variance'],\n",
    "                          gaus_bayes['n_classes'], n_feat)\n",
    "        prob = pr + like\n",
    "        max_prob_class_idx = np.argmax(prob)\n",
    "        predictions.append(gaus_bayes['unique_labels'][max_prob_class_idx])\n",
    "    return np.array(predictions)\n",
    "\n",
    "\n",
    "def evaluate(test_data: pd.DataFrame, label: str, gaus_bayes: Dict[str, np.ndarray]) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of a Gaussian Naive Bayes model on a test dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_data : DataFrame\n",
    "        The test dataset.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "    gaus_bayes : dict\n",
    "        A dictionary containing the parameters of the trained Gaussian Naive Bayes model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    accuracy : float\n",
    "        The accuracy of the Gaussian Naive Bayes model on the test dataset.\n",
    "    \"\"\"\n",
    "    gaus_pred = predict(test_data, gaus_bayes)\n",
    "    correct_predict = 0\n",
    "    wrong_predict = 0\n",
    "    for index in tqdm(range(len(test_data.index))):\n",
    "        if gaus_pred[index] == test_data[label].iloc[index]:\n",
    "            correct_predict += 1\n",
    "        else:\n",
    "            wrong_predict += 1\n",
    "    accuracy = correct_predict / (correct_predict + wrong_predict)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolas/Desktop/Phd/Intro_to_ML_25-26/venv/lib/python3.7/site-packages/ipykernel_launcher.py:29: RuntimeWarning: divide by zero encountered in log\n",
      "100%|██████████| 3611/3611 [00:00<00:00, 163490.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.903073940736638"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(test_data, 'Class', gaus_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "\n",
    "cols = list(set(train_data.columns) - {'Class'})\n",
    "\n",
    "nb.fit(train_data[cols], train_data['Class'])\n",
    "\n",
    "pred = nb.predict(test_data[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.903073940736638\n"
     ]
    }
   ],
   "source": [
    "print((pred == label_test.values).sum()/len(pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
